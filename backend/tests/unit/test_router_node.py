"""Unit tests for Router node (intent classification)."""

import pytest
from unittest.mock import AsyncMock, patch, MagicMock

from app.rag.nodes.router_node import classify_intent
from app.rag.utils.state import GraphState
from app.schemas.llm_responses import IntentClassification


class TestRouterNode:
    """Unit tests for classify_intent() node."""

    @pytest.mark.asyncio
    async def test_classify_intent_chitchat(self):
        """Router correctly classifies chitchat intent."""
        state: GraphState = {
            "user_query": "Hello! How are you today?",
            "user_id": "user123",
            "conversation_history": []
        }

        # Mock LLM client to return chitchat classification
        mock_llm_client = MagicMock()
        mock_llm_client.ainvoke_gemini_structured = AsyncMock(
            return_value=IntentClassification(
                intent="chitchat",
                confidence=0.95,
                reasoning="User is greeting, no specific question or request"
            )
        )

        with patch("app.rag.nodes.router_node.LLMClient", return_value=mock_llm_client):
            result_state = await classify_intent(state)

        # Verify intent classification
        assert result_state["intent"] == "chitchat"
        assert result_state["metadata"]["intent_confidence"] == 0.95
        assert "greeting" in result_state["metadata"]["intent_reasoning"].lower()

        # Verify LLM called correctly
        mock_llm_client.ainvoke_gemini_structured.assert_called_once()
        call_args = mock_llm_client.ainvoke_gemini_structured.call_args
        assert call_args.kwargs["schema"] == IntentClassification
        assert call_args.kwargs["temperature"] == 0.3

    @pytest.mark.asyncio
    async def test_classify_intent_qa(self):
        """Router correctly classifies Q&A intent."""
        state: GraphState = {
            "user_query": "What is dependency injection in FastAPI?",
            "user_id": "user123",
            "conversation_history": []
        }

        mock_llm_client = MagicMock()
        mock_llm_client.ainvoke_gemini_structured = AsyncMock(
            return_value=IntentClassification(
                intent="qa",
                confidence=0.92,
                reasoning="User asks a specific factual question requiring knowledge retrieval"
            )
        )

        with patch("app.rag.nodes.router_node.LLMClient", return_value=mock_llm_client):
            result_state = await classify_intent(state)

        assert result_state["intent"] == "qa"
        assert result_state["metadata"]["intent_confidence"] == 0.92
        assert "knowledge retrieval" in result_state["metadata"]["intent_reasoning"]

    @pytest.mark.asyncio
    async def test_classify_intent_linkedin(self):
        """Router correctly classifies LinkedIn post generation intent."""
        state: GraphState = {
            "user_query": "Write a LinkedIn post about async programming in Python",
            "user_id": "user123",
            "conversation_history": []
        }

        mock_llm_client = MagicMock()
        mock_llm_client.ainvoke_gemini_structured = AsyncMock(
            return_value=IntentClassification(
                intent="linkedin",
                confidence=0.98,
                reasoning="User explicitly requests LinkedIn post creation"
            )
        )

        with patch("app.rag.nodes.router_node.LLMClient", return_value=mock_llm_client):
            result_state = await classify_intent(state)

        assert result_state["intent"] == "linkedin"
        assert result_state["metadata"]["intent_confidence"] == 0.98
        assert "linkedin" in result_state["metadata"]["intent_reasoning"].lower()

    @pytest.mark.asyncio
    async def test_classify_intent_with_conversation_history(self):
        """Router uses conversation history for context."""
        state: GraphState = {
            "user_query": "Tell me more about that",
            "user_id": "user123",
            "conversation_history": [
                {"role": "user", "content": "What is FastAPI?"},
                {"role": "assistant", "content": "FastAPI is a modern web framework..."}
            ]
        }

        mock_llm_client = MagicMock()
        mock_llm_client.ainvoke_gemini_structured = AsyncMock(
            return_value=IntentClassification(
                intent="qa",
                confidence=0.88,
                reasoning="Follow-up question from previous context about FastAPI"
            )
        )

        with patch("app.rag.nodes.router_node.LLMClient", return_value=mock_llm_client):
            with patch("app.rag.nodes.router_node.render_prompt") as mock_render:
                mock_render.return_value = "Mocked prompt"
                result_state = await classify_intent(state)

                # Verify conversation history passed to template
                mock_render.assert_called_once()
                call_args = mock_render.call_args
                assert call_args.args[0] == "query_router.jinja2"
                assert call_args.kwargs["conversation_history"] == state["conversation_history"]
                assert call_args.kwargs["user_query"] == "Tell me more about that"

        assert result_state["intent"] == "qa"

    @pytest.mark.asyncio
    async def test_classify_intent_empty_query(self):
        """Router handles empty user query gracefully."""
        state: GraphState = {
            "user_query": "",
            "user_id": "user123",
            "conversation_history": []
        }

        mock_llm_client = MagicMock()
        mock_llm_client.ainvoke_gemini_structured = AsyncMock(
            return_value=IntentClassification(
                intent="chitchat",
                confidence=0.5,
                reasoning="Empty query, treating as chitchat"
            )
        )

        with patch("app.rag.nodes.router_node.LLMClient", return_value=mock_llm_client):
            result_state = await classify_intent(state)

        assert result_state["intent"] == "chitchat"
        assert result_state["metadata"]["intent_confidence"] == 0.5

    @pytest.mark.asyncio
    async def test_classify_intent_missing_user_query(self):
        """Router handles missing user_query field."""
        state: GraphState = {
            "user_id": "user123",
            "conversation_history": []
        }  # No user_query

        mock_llm_client = MagicMock()
        mock_llm_client.ainvoke_gemini_structured = AsyncMock(
            return_value=IntentClassification(
                intent="chitchat",
                confidence=0.3,
                reasoning="No query provided"
            )
        )

        with patch("app.rag.nodes.router_node.LLMClient", return_value=mock_llm_client):
            result_state = await classify_intent(state)

        # Should handle gracefully (empty string default)
        assert "intent" in result_state
        assert result_state["intent"] == "chitchat"

    @pytest.mark.asyncio
    async def test_classify_intent_preserves_existing_state(self):
        """Router preserves all existing state fields."""
        state: GraphState = {
            "user_query": "What is FastAPI?",
            "user_id": "user123",
            "conversation_history": [],
            "retrieved_chunks": [{"chunk_id": "chunk1"}],  # Existing field
            "metadata": {"existing_key": "existing_value"}  # Existing metadata
        }

        mock_llm_client = MagicMock()
        mock_llm_client.ainvoke_gemini_structured = AsyncMock(
            return_value=IntentClassification(
                intent="qa",
                confidence=0.9,
                reasoning="Question about FastAPI"
            )
        )

        with patch("app.rag.nodes.router_node.LLMClient", return_value=mock_llm_client):
            result_state = await classify_intent(state)

        # Verify existing fields preserved
        assert result_state["user_query"] == "What is FastAPI?"
        assert result_state["user_id"] == "user123"
        assert result_state["retrieved_chunks"] == [{"chunk_id": "chunk1"}]

        # Verify new fields added
        assert result_state["intent"] == "qa"
        assert result_state["metadata"]["existing_key"] == "existing_value"
        assert result_state["metadata"]["intent_confidence"] == 0.9

    @pytest.mark.asyncio
    async def test_classify_intent_llm_error_propagates(self):
        """Router propagates LLM errors without catching."""
        state: GraphState = {
            "user_query": "Test query",
            "user_id": "user123",
            "conversation_history": []
        }

        mock_llm_client = MagicMock()
        mock_llm_client.ainvoke_gemini_structured = AsyncMock(
            side_effect=Exception("LLM API error")
        )

        with patch("app.rag.nodes.router_node.LLMClient", return_value=mock_llm_client):
            with pytest.raises(Exception, match="LLM API error"):
                await classify_intent(state)

    @pytest.mark.asyncio
    async def test_classify_intent_low_confidence(self):
        """Router handles low confidence classifications."""
        state: GraphState = {
            "user_query": "Hmm not sure what I want",
            "user_id": "user123",
            "conversation_history": []
        }

        mock_llm_client = MagicMock()
        mock_llm_client.ainvoke_gemini_structured = AsyncMock(
            return_value=IntentClassification(
                intent="chitchat",
                confidence=0.4,  # Low confidence
                reasoning="Ambiguous query, defaulting to chitchat"
            )
        )

        with patch("app.rag.nodes.router_node.LLMClient", return_value=mock_llm_client):
            result_state = await classify_intent(state)

        # Should still classify even with low confidence
        assert result_state["intent"] == "chitchat"
        assert result_state["metadata"]["intent_confidence"] == 0.4
        assert "ambiguous" in result_state["metadata"]["intent_reasoning"].lower()
